/**
   This file is part of opensearch.
   Copyright Â© 2009, Dansk Bibliotekscenter a/s,
   Tempovej 7-11, DK-2750 Ballerup, Denmark. CVR: 15149043

   opensearch is free software: you can redistribute it and/or modify
   it under the terms of the GNU General Public License as published by
   the Free Software Foundation, either version 3 of the License, or
   (at your option) any later version.

   opensearch is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
   GNU General Public License for more details.

   You should have received a copy of the GNU General Public License
   along with opensearch.  If not, see <http://www.gnu.org/licenses/>.
*/

/**
 * \file DatadockMain.java
 * \brief
 */


package dk.dbc.opensearch.datadock;


import dk.dbc.commons.db.OracleDBPooledConnection;
import dk.dbc.commons.db.PostgresqlDBConnection;
import dk.dbc.commons.os.FileHandler;
import dk.dbc.opensearch.config.DataBaseConfig;
import dk.dbc.opensearch.config.DatadockConfig;
import dk.dbc.opensearch.config.HarvesterConfig;
import dk.dbc.opensearch.db.IProcessqueue;
import dk.dbc.opensearch.db.Processqueue;
import dk.dbc.opensearch.fedora.FedoraObjectRepository;
import dk.dbc.opensearch.fedora.IObjectRepository;
import dk.dbc.opensearch.fedora.ObjectRepositoryException;
import dk.dbc.opensearch.harvest.ESHarvest;
import dk.dbc.opensearch.harvest.FileHarvest;
import dk.dbc.opensearch.harvest.FileHarvestLight;
import dk.dbc.opensearch.harvest.HarvesterIOException;
import dk.dbc.opensearch.harvest.IHarvest;
import dk.dbc.opensearch.helpers.Log4jConfiguration;
import dk.dbc.opensearch.pluginframework.PluginException;
import dk.dbc.opensearch.pluginframework.PluginResolver;
import dk.dbc.opensearch.pluginframework.FlowMapCreator;
import dk.dbc.opensearch.pluginframework.PluginTask;
import java.io.File;
import java.io.IOException;
import java.lang.reflect.InvocationTargetException;

import java.sql.SQLException;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;
import java.util.Properties;
import java.util.Map;
import java.util.List;
import javax.xml.parsers.ParserConfigurationException;

import oracle.jdbc.pool.OracleDataSource;

import org.apache.commons.configuration.ConfigurationException;
import org.apache.log4j.ConsoleAppender;
import org.apache.log4j.Logger;
import org.apache.log4j.SimpleLayout;

import org.xml.sax.SAXException;


/**
 * The Main method of the datadock. It secures all necessary
 * resources for the program, starts the datadockManager and then
 * closes stdin and stdout thus closing connection to the console.
 *
 * It also adds a shutdown hook to the JVM so orderly shutdown is
 * accompleshed when the process is killed.
 */
public class DatadockMain
{
    /**
     *  Private enum used to differentiate between various harvester types
     */
    private enum HarvestType
    {
	ESHarvest,
	FileHarvest,
	FileHarvestLight;
    }

    private final static Logger log = Logger.getLogger( DatadockMain.class );

    
    /** TODO: what is the purpose of rootAppender and startupAppender wrt the startup in this class*/
    private final static ConsoleAppender startupAppender = new ConsoleAppender( new SimpleLayout() );

    private static final String logConfiguration = "log4j_datadock.xml";

    protected boolean shutdownRequested = false;
    
    static DatadockManager datadockManager;
    
    static FlowMapCreator flowMapCreator = null;


    private final int queueSize;
    private final int corePoolSize;
    private final int maxPoolSize;
    private final long keepAliveTime;
    private final int pollTime;
    private final File pluginFlowXmlPath;
    private final File pluginFlowXsdPath;
    private static HarvestType defaultHarvestType = HarvestType.FileHarvest;
    static java.util.Date startTime = null;
    private boolean terminateOnZeroSubmitted = false;

    
    public DatadockMain()  throws ConfigurationException
    {
        pollTime = DatadockConfig.getMainPollTime();
        queueSize = DatadockConfig.getQueueSize();
        corePoolSize = DatadockConfig.getCorePoolSize();
        maxPoolSize = DatadockConfig.getMaxPoolSize();
        keepAliveTime = DatadockConfig.getKeepAliveTime();

        log.debug(  String.format( "Starting Datadock with pollTime = %s", pollTime ) );
        log.debug(  String.format( "Starting Datadock with queueSize = %s", queueSize ) );
        log.debug(  String.format( "Starting Datadock with corePoolSize = %s", corePoolSize ) );
        log.debug(  String.format( "Starting Datadock with maxPoolSize = %s", maxPoolSize ) );
        log.debug(  String.format( "Starting Datadock with keepAliveTime = %s", keepAliveTime ) );

        pluginFlowXmlPath = DatadockConfig.getPluginFlowXmlPath();
        pluginFlowXsdPath = DatadockConfig.getPluginFlowXsdPath();
        if( null == pluginFlowXmlPath || null == pluginFlowXsdPath )
        {
            throw new ConfigurationException( "Failed to initialize configuration values for File objects properly (pluginFlowXmlPath or pluginFlowXsdPath)" );
        }
        log.debug(  String.format( "Starting Datadock with pluginFlowXmlPath = %s", pluginFlowXmlPath ) );
        log.debug(  String.format( "Starting Datadock with pluginFlowXsdPath = %s", pluginFlowXsdPath) );
    }


    /**
     *  Gets the type of a
     * {@link dk.dbc.opensearch.harvest.IHarvest Harvester} from a
     * command line parameter or, if that fails, the type of a default harvester
     * specified in the class
     */
    private HarvestType getHarvesterType()
    {
        log.trace( "Trying to get harvester type from commandline" );
        String harvestTypeFromCmdLine = System.getProperty( "harvester" );
        log.debug( String.format( "Found this harvester: %s", harvestTypeFromCmdLine ) );

        HarvestType harvestType = null;
        if ( harvestTypeFromCmdLine == null || harvestTypeFromCmdLine.isEmpty() )
        {
            // Only set to default harvester if none is given on commandline
            harvestType = defaultHarvestType;
        }
        else
        {
            harvestType = harvestTypeFromCmdLine.equals( "ESHarvest" ) ? HarvestType.ESHarvest : harvestType;
            harvestType = harvestTypeFromCmdLine.equals( "FileHarvest" ) ? HarvestType.FileHarvest : harvestType;
            harvestType = harvestTypeFromCmdLine.equals( "FileHarvestLight" ) ? HarvestType.FileHarvestLight : harvestType;

            if ( harvestType == null )
            {
            throw new IllegalArgumentException( String.format( "Unknown harvestType: %s", harvestTypeFromCmdLine ) );
            }
        }
	
        log.debug( String.format( "initialized harvester with type: %s", harvestType ) );
        return harvestType;

    }

    /**
     * Reads command line arguments and initializes the server mode
     */
    private void setServerMode()
    {
        String mode = System.getProperty( "shutDownOnJobsDone" );
        if( null != mode && mode.equals( "true" ) )
        {
            this.terminateOnZeroSubmitted = true;
        }
    }

    /**
     * This method does the actual work of nudging the datadockmanager to get
     * on with processing data from the harvester. If any exceptions are thrown
     * from the manager, this method will issue a shutdown, and exit.
     *
     * @return the number of jobs that have been submitted for processing up until a shutdown request
     */
    private int runServer()
    {
        int mainJobsSubmitted = 0;
        try
        {
            while( !isShutdownRequested() )
            {
                log.trace( "DatadockMain calling datadockManager update" );
                long timer = System.currentTimeMillis();
                int jobsSubmitted = datadockManager.update();
                log.debug( String.format( "%s jobs submitted according to the DatadockManager", jobsSubmitted ) );
                timer = System.currentTimeMillis() - timer;
                mainJobsSubmitted += jobsSubmitted;
                if( jobsSubmitted > 0 )
                {
                    log.info( String.format( "%1$d Jobs submitted in %2$d ms - %3$f jobs/s", jobsSubmitted, timer, jobsSubmitted / (timer / 1000.0) ) );
                }
                else
                {
                    log.info( String.format( "%1$d Jobs submitted in %2$d ms - ", jobsSubmitted, timer ) );
                    if( terminateOnZeroSubmitted )
                    {
                        log.info( "Program set to terminate on empty job queue. Shutting down now" );
                        this.shutdown();
                    }
                    else
                    {
                        Thread.currentThread();
                        Thread.sleep( this.pollTime );
                    }
                }
            }
        }
        catch( HarvesterIOException hioe )
        {
            String fatal = String.format( "A fatal error occured in the communication with the database: %s", hioe.getMessage() );
            log.fatal( fatal, hioe );
        }
        catch( InterruptedException ie )
        {
            log.fatal( String.format( "InterruptedException caught in Main.runServer: %s", ie.getMessage() ), ie  );
        }
        catch( RuntimeException re )
        {
            log.fatal( String.format( "RuntimeException caught in Main.runServer: %s", re.getMessage() ), re );
        }
        catch( Exception e )
        {
            log.fatal( String.format( "Exception caught in Main.runServer: %s", e.getMessage() ), e );
        }
//        finally
//        {
//            this.shutdown();
//        }
        log.debug( String.format( "Total # jobs submitted to main: %s", mainJobsSubmitted ) );
        return mainJobsSubmitted;
    }

    /**
     * The shutdown hook. This method is called when the program catches a
     * kill signal.
     */
    private void shutdown()
    {
        this.shutdownRequested = true;

        try
        {
            log.info( "Shutting down." );
            datadockManager.shutdown();
        }
        catch( InterruptedException e )
        {
            log.error( String.format(  "Interrupted while waiting on main daemon thread to complete: %s", e.getMessage() ) );
            System.exit( -1 );
        }
        catch( HarvesterIOException hioe )
        {
            log.fatal( String.format( "Some error occured while shutting down the harvester: %s", hioe.getMessage() ) );
            System.exit( -1 );
        }
        catch( NullPointerException npe )
        {
            log.fatal( "DatadockManager does not seem to have been started or it crashed. Shutting down with the risk of inconsistencies" );
            System.exit( -1 );
        }
        
        log.info( "Exiting normally." );
    }


    /**
     * Getter method for shutdown signal.
     */
    public boolean isShutdownRequested()
    {
        return this.shutdownRequested;
    }


    /**
     * Daemonizes the program, ie. disconnects from the console and
     * creates a pidfile.
     */
    private void daemonize()
    {
        String pidFile = System.getProperty( "daemon.pidfile" );
        FileHandler.getFile( pidFile ).deleteOnExit();
        System.out.close();
        System.err.close();
    }


    /**
     * Adds the shutdownhook.
     */
    protected void addDaemonShutdownHook()
    {
        Runtime.getRuntime().addShutdownHook( new Thread() {
                @Override
                public void run()
                {
                    shutdown();
                }
            } );
    }


    private void collectStatistics( long mainTimer, int mainJobsSubmitted )
    {
        mainTimer = System.currentTimeMillis() - mainTimer;
        if( mainJobsSubmitted > 0 )
        {
            log.info( String.format( "Total: %1$d Jobs submitted in %2$d ms - %3$f jobs/s", mainJobsSubmitted, mainTimer, mainJobsSubmitted / (mainTimer / 1000.0) ) );
        }
        else
        {
            log.info( String.format( "Total: %1$d Jobs submitted in %2$d ms - ", mainJobsSubmitted, mainTimer ) );
        }
    }


    private IHarvest initializeHarvester() throws SQLException, IllegalArgumentException, ConfigurationException, SAXException, HarvesterIOException, IOException
    {
        log.trace( "Getting harvester type" );
        HarvestType harvestType = this.getHarvesterType();

        IHarvest harvester;
        switch( harvestType )
        {
            case ESHarvest:
                harvester = this.selectESHarvester();
                break;
            case FileHarvest:
                log.trace( "selecting FileHarvest" );
                harvester = new FileHarvest();
                break;
            case FileHarvestLight:
                log.trace( "selecting FileHarvestLight" );
                harvester = new FileHarvestLight();
                break;
            default:
                log.warn( "no harvester explicitly selected, and default type failed. This should not happen, but I'll default to FileHarvester" );
                harvester = new FileHarvest();
        }
        return harvester;
    }

    private IHarvest selectESHarvester() throws ConfigurationException, SQLException, HarvesterIOException
    {
        String dataBaseName = DataBaseConfig.getOracleDataBaseName();
        String oracleCacheName = DataBaseConfig.getOracleCacheName();
        String oracleUrl = DataBaseConfig.getOracleUrl();
        String oracleUser = DataBaseConfig.getOracleUserID();
        String oraclePassWd = DataBaseConfig.getOraclePassWd();
        String minLimit = DataBaseConfig.getOracleMinLimit();
        String maxLimit = DataBaseConfig.getOracleMaxLimit();
        String initialLimit = DataBaseConfig.getOracleInitialLimit();
        String connectionWaitTimeout = DataBaseConfig.getOracleConnectionWaitTimeout();

        boolean usePriorityFlag = HarvesterConfig.getPriorityFlag();

        log.info( String.format( "DB Url : %s ", oracleUrl ) );
        log.info( String.format( "DB User: %s ", oracleUser ) );
        OracleDataSource ods;
        try
        {
            ods = new OracleDataSource();

            // set db-params:
            ods.setURL( oracleUrl );
            ods.setUser( oracleUser );
            ods.setPassword( oraclePassWd );

            // set db-cache-params:
            ods.setConnectionCachingEnabled( true ); // connection pool

            // set the cache name
            ods.setConnectionCacheName( oracleCacheName );

            // set cache properties:
            Properties cacheProperties = new Properties();

            cacheProperties.setProperty( "MinLimit", minLimit );
            cacheProperties.setProperty( "MaxLimit", maxLimit );
            cacheProperties.setProperty( "InitialLimit", initialLimit );
            cacheProperties.setProperty( "ConnectionWaitTimeout", connectionWaitTimeout );
            cacheProperties.setProperty( "ValidateConnection", "true" );

            ods.setConnectionCacheProperties( cacheProperties );

        }
        catch( SQLException sqle )
        {
            String errorMsg = "An SQL error occured during the setup of the OracleDataSource";
            log.fatal( errorMsg, sqle );
            throw sqle;
        }

        OracleDBPooledConnection connectionPool = new OracleDBPooledConnection( oracleCacheName, ods );

        return new ESHarvest( connectionPool, dataBaseName, usePriorityFlag );

    }


    private void initializeServices() throws ObjectRepositoryException, InstantiationException, IllegalAccessException, PluginException, HarvesterIOException, IllegalStateException, ParserConfigurationException, IOException, IllegalArgumentException, SQLException, InvocationTargetException, SAXException, ConfigurationException, ClassNotFoundException
    {
        log.trace( "Initializing process queue" );
        String driver = DataBaseConfig.getPostgresqlDriver();
        String url    = DataBaseConfig.getPostgresqlUrl();
        String userId = DataBaseConfig.getPostgresqlUserID();
        String passwd = DataBaseConfig.getPostgresqlPassWd();
        PostgresqlDBConnection conn = new PostgresqlDBConnection( userId, passwd, url, driver );
        IProcessqueue processqueue = new Processqueue( conn );

        log.trace( "Initializing plugin resolver" );
        IObjectRepository repository = new FedoraObjectRepository();
        PluginResolver pluginResolver = new PluginResolver( repository );
        flowMapCreator = new FlowMapCreator( this.pluginFlowXmlPath, this.pluginFlowXsdPath );
        Map<String, List<PluginTask>> flowMap = flowMapCreator.createMap( pluginResolver, repository );

        log.trace( "Initializing harvester" );
        IHarvest harvester = this.initializeHarvester();

        log.trace( "Initializing the DatadockPool" );
        LinkedBlockingQueue<Runnable> queue = new LinkedBlockingQueue<Runnable>( this.queueSize );
        ThreadPoolExecutor threadpool = new ThreadPoolExecutor( this.corePoolSize, this.maxPoolSize, this.keepAliveTime, TimeUnit.SECONDS, queue );
        DatadockPool datadockPool = new DatadockPool( threadpool, processqueue, harvester, flowMap );

        log.trace( "Initializing the DatadockManager" );
        datadockManager = new DatadockManager( datadockPool, harvester, flowMap );
    }


    /**
     * The datadocks main method.
     * Starts the datadock and starts the datadockManager.
     */
    public static void main(String[] args)
    {
        try
        {
            Log4jConfiguration.configure( logConfiguration );
        }
        catch( ConfigurationException ex )
        {
            System.out.println( String.format( "Logger could not be configured, will continue without logging: %s", ex.getMessage() ) );
        }

        DatadockMain serverInstance = null;
        try
        {
            serverInstance = new DatadockMain();
            serverInstance.setServerMode();
        }
        catch( ConfigurationException ex )
        {
            String error = String.format( "Could not get configure DatadockMain object: %s", ex.getMessage() );
            log.fatal( error, ex );
            //we cannot guarantee a serverInstance to call shutdown on:
            System.exit( -1 );
        }

        log.removeAppender( "RootConsoleAppender" );
        log.addAppender( startupAppender );

        try
        {
            serverInstance.initializeServices();
        }
        catch ( Exception e )
        {
            System.out.println( "Startup failed." + e.getMessage() );
            log.fatal( String.format( "Startup failed: %s", e.getMessage() ), e );
            serverInstance.shutdown();

        }
        finally
        {
            log.removeAppender( startupAppender );
        }

        log.info( "Daemonizing Datadock server" );
        serverInstance.daemonize();
        serverInstance.addDaemonShutdownHook();

        log.info( "Starting processing of data" );
        long mainTimer = System.currentTimeMillis();
        int mainJobsSubmitted = serverInstance.runServer();

        log.info( "Collecting and printing processing statistics" );
        serverInstance.collectStatistics( mainTimer, mainJobsSubmitted );
    }
}
